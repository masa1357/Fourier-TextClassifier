{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フーリエ変換を用いたテキストのクラス分類器を実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import MeCab\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.fft import fft\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: c:\\Users\\zigza\\OneDrive\\ドキュメント\\git\\Fourier-TextClassifier\n",
      "DATA_PATH: c:\\Users\\zigza\\OneDrive\\ドキュメント\\git\\Fourier-TextClassifier\\data\n",
      "MODEL_PATH: c:\\Users\\zigza\\OneDrive\\ドキュメント\\git\\Fourier-TextClassifier\\model\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = Path.cwd().parent\n",
    "DATA_PATH = BASE_PATH / \"data\"\n",
    "MODEL_PATH = BASE_PATH / \"model\"\n",
    "print(f\"BASE_PATH: {BASE_PATH}\")\n",
    "print(f\"DATA_PATH: {DATA_PATH}\")\n",
    "print(f\"MODEL_PATH: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-06 20:19:07,887 : python.util : INFO : 34 : Test_message\n"
     ]
    }
   ],
   "source": [
    "import python.util as util\n",
    "\n",
    "importlib.reload(util)\n",
    "# ? logger読み込み\n",
    "name = \"15-Ensemble\"\n",
    "logger = util.set_logger()\n",
    "# ? seed値固定\n",
    "seed = 42\n",
    "util.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASETS\n",
    "今回は<a href=\"https://huggingface.co/datasets/stanfordnlp/imdb\">Large Movie Review Dataset</a>を用いる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('c:/Users/zigza/OneDrive/ドキュメント/git/Fourier-TextClassifier/data')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zigza\\OneDrive\\ドキュメント\\git\\Fourier-TextClassifier\\data\\train.pkl\n"
     ]
    }
   ],
   "source": [
    "print(DATA_PATH / \"train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO: データ読み込み\n",
    "# pklの読み込み\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"stanfordnlp/imdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds[\"train\"]\n",
    "test = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zigza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zigza\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def dataclean(sentence):\n",
    "    # stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    # logger.info(f\"before: {sentence}\")\n",
    "    # 1. 記号の削除\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    # 2. 小文字化\n",
    "    sentence = sentence.lower()\n",
    "    # 3. トークン化\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    # 4. stopwordsの削除\n",
    "    sentence = [\n",
    "        word for word in sentence if not word in set(stopwords.words(\"english\"))\n",
    "    ]\n",
    "    # 5. レマタイズ\n",
    "    lemma = nltk.WordNetLemmatizer()\n",
    "    sentence = [lemma.lemmatize(word) for word in sentence]\n",
    "    # 結合\n",
    "    sentence = \" \".join(sentence)\n",
    "    # logger.info(f\"after: {sentence}\")\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listをpd.DataFrameに変換\n",
    "train_df = pd.DataFrame(train)\n",
    "test_df = pd.DataFrame(test)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理\n",
    "# with util.timer(\"train前処理\"):\n",
    "#     train_df['text'] = train_df['text'].apply(dataclean)\n",
    "\n",
    "# with util.timer(\"train前処理\"):\n",
    "#     train_df[\"text\"] = train_df[\"text\"].progress_apply(dataclean)\n",
    "\n",
    "# train_df.head()\n",
    "# train_df.to_csv(DATA_PATH / \"train.csv\", index=False)\n",
    "train_df = pd.read_csv(DATA_PATH / \"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df[\"text\"] = test_df[\"text\"].apply(dataclean)\n",
    "# test_df.to_csv(DATA_PATH / \"test.csv\", index=False)\n",
    "\n",
    "test_df = pd.read_csv(DATA_PATH / \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フーリエ変換を用いたテキストエンコーディング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ベクトル化\n",
    "今回はTF-IDFを用いる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizer = TfidfVectorizer()\n",
    "# X_train = vectorizer.fit_transform(train_df[\"text\"]).toarray()\n",
    "# X_test = vectorizer.transform(test_df[\"text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = train_df[\"text\"].apply(word_tokenize)\n",
    "test_tokens = test_df[\"text\"].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Train Vectors: 100%|██████████| 25000/25000 [00:06<00:00, 3614.89it/s]\n",
      "Calculating Test Vectors: 100%|██████████| 25000/25000 [00:06<00:00, 3747.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Word2Vecモデルの学習\n",
    "model = Word2Vec(\n",
    "    sentences=train_tokens, vector_size=100, window=5, min_count=1, workers=4\n",
    ")\n",
    "\n",
    "\n",
    "# 平均ベクトルを計算する関数\n",
    "def compute_average_vector(tokens, model):\n",
    "    valid_tokens = [token for token in tokens if token in model.wv]\n",
    "    if not valid_tokens:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(model.wv[valid_tokens], axis=0)\n",
    "\n",
    "\n",
    "# 訓練データとテストデータのベクトルを計算\n",
    "X_train = np.array(\n",
    "    [\n",
    "        compute_average_vector(tokens, model)\n",
    "        for tokens in tqdm(train_tokens, desc=\"Calculating Train Vectors\")\n",
    "    ]\n",
    ")\n",
    "X_test = np.array(\n",
    "    [\n",
    "        compute_average_vector(tokens, model)\n",
    "        for tokens in tqdm(test_tokens, desc=\"Calculating Test Vectors\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FFTの適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FFT on Train Data: 100%|██████████| 25000/25000 [00:00<00:00, 92927.81it/s]\n",
      "FFT on Test Data: 100%|██████████| 25000/25000 [00:00<00:00, 127532.18it/s]\n"
     ]
    }
   ],
   "source": [
    "fft_train = [fft(x) for x in tqdm(X_train, desc=\"FFT on Train Data\")]\n",
    "fft_test = [fft(x) for x in tqdm(X_test, desc=\"FFT on Test Data\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIFVの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_membership(value, thresholds):\n",
    "    if value <= thresholds[0]:\n",
    "        return 0.0\n",
    "    elif value >= thresholds[1]:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return (value - thresholds[0]) / (thresholds[1] - thresholds[0])\n",
    "\n",
    "\n",
    "def calculate_tifv(fft_results):\n",
    "    tifv_results = []\n",
    "    for fft_result in fft_results:\n",
    "        tifv_values = []\n",
    "        for value in fft_result:\n",
    "            membership_real = fuzzy_membership(value.real, [0, 1])\n",
    "            membership_imag = fuzzy_membership(value.imag, [0, 1])\n",
    "            tifv = (membership_real + (1 - membership_imag)) / 2\n",
    "            tifv_values.append(tifv)\n",
    "        tifv_results.append(tifv_values)\n",
    "    return tifv_results\n",
    "\n",
    "\n",
    "tifv_train = calculate_tifv(fft_train)\n",
    "tifv_test = calculate_tifv(fft_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ロジスティック回帰による分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.51156\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TIFVの平均値を特徴量として使用\n",
    "X_train_tifv = np.array([np.mean(tifv) for tifv in tifv_train]).reshape(-1, 1)\n",
    "X_test_tifv = np.array([np.mean(tifv) for tifv in tifv_test]).reshape(-1, 1)\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# モデルのトレーニング\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tifv, y_train)\n",
    "\n",
    "# モデルの予測\n",
    "y_pred = model.predict(X_test_tifv)\n",
    "\n",
    "# 精度の評価\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # サンプルテキスト\n",
    "# texts = [\n",
    "#     \"I love this product!\",\n",
    "#     \"This is the worst experience ever.\",\n",
    "#     \"It's okay, not great but not terrible.\",\n",
    "# ]\n",
    "\n",
    "# # トークン化とTF-IDFベクトル化\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "# # パディング\n",
    "# max_length = max([len(x) for x in X])\n",
    "# padded_X = np.array([np.pad(x, (0, max_length - len(x)), \"constant\") for x in X])\n",
    "\n",
    "# # FFTの適用\n",
    "# fft_results = [fft(x) for x in padded_X]\n",
    "\n",
    "\n",
    "# # ファジィメンバーシップ関数の定義\n",
    "# def fuzzy_membership(value, thresholds):\n",
    "#     if value <= thresholds[0]:\n",
    "#         return 0.0\n",
    "#     elif value >= thresholds[1]:\n",
    "#         return 1.0\n",
    "#     else:\n",
    "#         return (value - thresholds[0]) / (thresholds[1] - thresholds[0])\n",
    "\n",
    "\n",
    "# # TIFVの計算\n",
    "# tifv_results = []\n",
    "# for fft_result in fft_results:\n",
    "#     tifv_values = []\n",
    "#     for value in fft_result:\n",
    "#         membership_real = fuzzy_membership(value.real, [0, 1])\n",
    "#         membership_imag = fuzzy_membership(value.imag, [0, 1])\n",
    "#         tifv = (membership_real + (1 - membership_imag)) / 2\n",
    "#         tifv_values.append(tifv)\n",
    "#     tifv_results.append(tifv_values)\n",
    "\n",
    "\n",
    "# # 感情分析\n",
    "# def sentiment_classification(tifv_values):\n",
    "#     mean_tifv = np.mean(tifv_values)\n",
    "#     if mean_tifv > 0.5:\n",
    "#         return \"Positive\"\n",
    "#     elif mean_tifv < 0.5:\n",
    "#         return \"Negative\"\n",
    "#     else:\n",
    "#         return \"Neutral\"\n",
    "\n",
    "\n",
    "# sentiments = [sentiment_classification(tifv) for tifv in tifv_results]\n",
    "# print(sentiments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
