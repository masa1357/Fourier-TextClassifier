{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フーリエ変換を用いたテキストのクラス分類器を実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "# import MeCab\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.fft import fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = Path.cwd().parent\n",
    "DATA_PATH = BASE_PATH / \"data\"\n",
    "MODEL_PATH = BASE_PATH / \"model\"\n",
    "print(f\"BASE_PATH: {BASE_PATH}\")\n",
    "print(f\"DATA_PATH: {DATA_PATH}\")\n",
    "print(f\"MODEL_PATH: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import python.util as util\n",
    "\n",
    "importlib.reload(util)\n",
    "# ? logger読み込み\n",
    "name = \"15-Ensemble\"\n",
    "logger = util.set_logger()\n",
    "# ? seed値固定\n",
    "seed = 42\n",
    "util.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASETS\n",
    "今回は<a href=\"https://huggingface.co/datasets/stanfordnlp/imdb\">Large Movie Review Dataset</a>を用いる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DATA_PATH / \"train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO: データ読み込み\n",
    "# pklの読み込み\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"stanfordnlp/imdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds[\"train\"]\n",
    "test = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "def dataclean(sentence):\n",
    "    # stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    # logger.info(f\"before: {sentence}\")\n",
    "    # 1. 記号の削除\n",
    "    sentence = re.sub(\"[^a-zA-Z]\", \" \", sentence)\n",
    "    # 2. 小文字化\n",
    "    sentence = sentence.lower()\n",
    "    # 3. トークン化\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    # 4. stopwordsの削除\n",
    "    sentence = [\n",
    "        word for word in sentence if not word in set(stopwords.words(\"english\"))\n",
    "    ]\n",
    "    # 5. レマタイズ\n",
    "    lemma = nltk.WordNetLemmatizer()\n",
    "    sentence = [lemma.lemmatize(word) for word in sentence]\n",
    "    # 結合\n",
    "    sentence = \" \".join(sentence)\n",
    "    # logger.info(f\"after: {sentence}\")\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#listをpd.DataFrameに変換\n",
    "train_df = pd.DataFrame(train)\n",
    "test_df = pd.DataFrame(test)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前処理\n",
    "with util.timer(\"train前処理\"):\n",
    "    train_df['text'] = train_df['text'].apply(dataclean)\n",
    "\n",
    "train_df.head()\n",
    "train_df.to_csv(DATA_PATH / \"train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"text\"] = test_df[\"text\"].apply(dataclean)\n",
    "test_df.to_csv(DATA_PATH / \"test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### フーリエ変換を用いたテキストエンコーディング"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ベクトル化\n",
    "今回はTF-IDFを用いる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_df[\"text\"]).toarray()\n",
    "X_test = vectorizer.transform(test_df[\"text\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FFTの適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fft_train = [fft(x) for x in X_train]\n",
    "fft_test = [fft(x) for x in X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIFVの計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_membership(value, thresholds):\n",
    "    if value <= thresholds[0]:\n",
    "        return 0.0\n",
    "    elif value >= thresholds[1]:\n",
    "        return 1.0\n",
    "    else:\n",
    "        return (value - thresholds[0]) / (thresholds[1] - thresholds[0])\n",
    "\n",
    "\n",
    "def calculate_tifv(fft_results):\n",
    "    tifv_results = []\n",
    "    for fft_result in fft_results:\n",
    "        tifv_values = []\n",
    "        for value in fft_result:\n",
    "            membership_real = fuzzy_membership(value.real, [0, 1])\n",
    "            membership_imag = fuzzy_membership(value.imag, [0, 1])\n",
    "            tifv = (membership_real + (1 - membership_imag)) / 2\n",
    "            tifv_values.append(tifv)\n",
    "        tifv_results.append(tifv_values)\n",
    "    return tifv_results\n",
    "\n",
    "\n",
    "tifv_train = calculate_tifv(fft_train)\n",
    "tifv_test = calculate_tifv(fft_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ロジスティック回帰による分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# TIFVの平均値を特徴量として使用\n",
    "X_train_tifv = np.array([np.mean(tifv) for tifv in tifv_train]).reshape(-1, 1)\n",
    "X_test_tifv = np.array([np.mean(tifv) for tifv in tifv_test]).reshape(-1, 1)\n",
    "\n",
    "y_train = train_df[\"label\"]\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "# モデルのトレーニング\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tifv, y_train)\n",
    "\n",
    "# モデルの予測\n",
    "y_pred = model.predict(X_test_tifv)\n",
    "\n",
    "# 精度の評価\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # サンプルテキスト\n",
    "# texts = [\n",
    "#     \"I love this product!\",\n",
    "#     \"This is the worst experience ever.\",\n",
    "#     \"It's okay, not great but not terrible.\",\n",
    "# ]\n",
    "\n",
    "# # トークン化とTF-IDFベクトル化\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "# # パディング\n",
    "# max_length = max([len(x) for x in X])\n",
    "# padded_X = np.array([np.pad(x, (0, max_length - len(x)), \"constant\") for x in X])\n",
    "\n",
    "# # FFTの適用\n",
    "# fft_results = [fft(x) for x in padded_X]\n",
    "\n",
    "\n",
    "# # ファジィメンバーシップ関数の定義\n",
    "# def fuzzy_membership(value, thresholds):\n",
    "#     if value <= thresholds[0]:\n",
    "#         return 0.0\n",
    "#     elif value >= thresholds[1]:\n",
    "#         return 1.0\n",
    "#     else:\n",
    "#         return (value - thresholds[0]) / (thresholds[1] - thresholds[0])\n",
    "\n",
    "\n",
    "# # TIFVの計算\n",
    "# tifv_results = []\n",
    "# for fft_result in fft_results:\n",
    "#     tifv_values = []\n",
    "#     for value in fft_result:\n",
    "#         membership_real = fuzzy_membership(value.real, [0, 1])\n",
    "#         membership_imag = fuzzy_membership(value.imag, [0, 1])\n",
    "#         tifv = (membership_real + (1 - membership_imag)) / 2\n",
    "#         tifv_values.append(tifv)\n",
    "#     tifv_results.append(tifv_values)\n",
    "\n",
    "\n",
    "# # 感情分析\n",
    "# def sentiment_classification(tifv_values):\n",
    "#     mean_tifv = np.mean(tifv_values)\n",
    "#     if mean_tifv > 0.5:\n",
    "#         return \"Positive\"\n",
    "#     elif mean_tifv < 0.5:\n",
    "#         return \"Negative\"\n",
    "#     else:\n",
    "#         return \"Neutral\"\n",
    "\n",
    "\n",
    "# sentiments = [sentiment_classification(tifv) for tifv in tifv_results]\n",
    "# print(sentiments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
