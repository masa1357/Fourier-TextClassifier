{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## フーリエ変換を用いたテキストのクラス分類器を実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import MeCab\n",
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import skfuzzy as fuzz\n",
    "from skfuzzy import control as ctrl\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.fft import fft\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE_PATH: /home/masa1357/Dockerdata/gitfile/Fourier-TextClassifier\n",
      "DATA_PATH: /home/masa1357/Dockerdata/gitfile/Fourier-TextClassifier/data\n",
      "MODEL_PATH: /home/masa1357/Dockerdata/gitfile/Fourier-TextClassifier/model\n"
     ]
    }
   ],
   "source": [
    "BASE_PATH = Path.cwd().parent\n",
    "DATA_PATH = BASE_PATH / \"data\"\n",
    "MODEL_PATH = BASE_PATH / \"model\"\n",
    "print(f\"BASE_PATH: {BASE_PATH}\")\n",
    "print(f\"DATA_PATH: {DATA_PATH}\")\n",
    "print(f\"MODEL_PATH: {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-08 17:25:08,822 : python.util : INFO : 34 : Test_message\n"
     ]
    }
   ],
   "source": [
    "import python.util as util\n",
    "\n",
    "importlib.reload(util)\n",
    "# ? logger読み込み\n",
    "name = \"15-Ensemble\"\n",
    "logger = util.set_logger()\n",
    "# ? seed値固定\n",
    "seed = 42\n",
    "util.set_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASETS\n",
    "今回は<a href=\"https://huggingface.co/datasets/stanfordnlp/imdb\">Large Movie Review Dataset</a>を用いる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/masa1357/Dockerdata/gitfile/Fourier-TextClassifier/data')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/masa1357/Dockerdata/gitfile/Fourier-TextClassifier/data/train.pkl\n"
     ]
    }
   ],
   "source": [
    "print(DATA_PATH / \"train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO: データ読み込み\n",
    "# pklの読み込み\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\n",
    "    \"stanfordnlp/imdb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = ds[\"train\"]\n",
    "test = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I rented I AM CURIOUS-YELLOW from my video sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"I Am Curious: Yellow\" is a risible and preten...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>If only to avoid making this type of film in t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This film was probably inspired by Godard's Ma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh, brother...after hearing about this ridicul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  I rented I AM CURIOUS-YELLOW from my video sto...      0\n",
       "1  \"I Am Curious: Yellow\" is a risible and preten...      0\n",
       "2  If only to avoid making this type of film in t...      0\n",
       "3  This film was probably inspired by Godard's Ma...      0\n",
       "4  Oh, brother...after hearing about this ridicul...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#listをpd.DataFrameに変換\n",
    "train_df = pd.DataFrame(train)\n",
    "test_df = pd.DataFrame(test)\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to /root/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.593\n",
      "F1 Score: 0.6858501343048566\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# データセットの読み込み（例としてCSVファイルから読み込む場合）\n",
    "# train_df = pd.read_csv('path_to_your_dataset.csv')\n",
    "\n",
    "# 例としてデータフレームを作成\n",
    "# data = {\"text\": [\"I love this movie!\", \"This is a terrible product.\"], \"label\": [1, 0]}\n",
    "# train_df = pd.DataFrame(data)\n",
    "\n",
    "# nltkのリソースをダウンロード（初回のみ必要）\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"sentiwordnet\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "\n",
    "# テキストの解析と品詞タグ付けを行う関数\n",
    "def pos_tagging(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    return pos_tags\n",
    "\n",
    "\n",
    "# 品詞タグをWordNetの形式に変換する関数\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# 単語の感情スコアを取得する関数\n",
    "def get_sentiment_score(word, pos):\n",
    "    wn_pos = get_wordnet_pos(pos)\n",
    "    if wn_pos is not None:\n",
    "        synsets = list(swn.senti_synsets(word, wn_pos))\n",
    "        if synsets:\n",
    "            # 最初のシノニムセットを使用して感情スコアを計算\n",
    "            swn_synset = synsets[0]\n",
    "            return swn_synset.pos_score(), swn_synset.neg_score()\n",
    "    return 0, 0\n",
    "\n",
    "\n",
    "# 各テキストの単語に対して感情スコアを取得する関数\n",
    "def sentiment_analysis(pos_tags):\n",
    "    sentiment_scores = []\n",
    "    for word, pos in pos_tags:\n",
    "        pos_score, neg_score = get_sentiment_score(word, pos)\n",
    "        sentiment_scores.append((word, pos, pos_score, neg_score))\n",
    "    return sentiment_scores\n",
    "\n",
    "\n",
    "# 時系列直観的ファジー値（TIFS）を計算する関数\n",
    "def calculate_tifs(sentiment_scores):\n",
    "    tifs = []\n",
    "    for i, (word, pos, pos_score, neg_score) in enumerate(sentiment_scores):\n",
    "        tifs.append((word, pos, pos_score, neg_score, i, 1 - (pos_score + neg_score)))\n",
    "    return tifs\n",
    "\n",
    "\n",
    "# FFTを適用する関数\n",
    "def apply_fft(tifs):\n",
    "    membership_values = [x[2] for x in tifs]  # メンバーシップ度（ポジティブスコア）\n",
    "    non_membership_values = [\n",
    "        x[3] for x in tifs\n",
    "    ]  # 非メンバーシップ度（ネガティブスコア）\n",
    "\n",
    "    # メンバーシップ値と非メンバーシップ値に対してFFTを適用\n",
    "    fft_membership = np.fft.fft(membership_values)\n",
    "    fft_non_membership = np.fft.fft(non_membership_values)\n",
    "\n",
    "    return fft_membership, fft_non_membership\n",
    "\n",
    "\n",
    "# 実数部分とパワー項を計算する関数\n",
    "def calculate_real_and_power(fft_results):\n",
    "    fft_membership, fft_non_membership = fft_results\n",
    "\n",
    "    # 実数部分を抽出\n",
    "    real_membership = np.real(fft_membership)\n",
    "    real_non_membership = np.real(fft_non_membership)\n",
    "\n",
    "    # パワー項を計算\n",
    "    power_membership = np.abs(fft_membership) ** 2\n",
    "    power_non_membership = np.abs(fft_non_membership) ** 2\n",
    "\n",
    "    # 合計パワー項を計算\n",
    "    total_power_membership = np.sum(power_membership)\n",
    "    total_power_non_membership = np.sum(power_non_membership)\n",
    "\n",
    "    return (\n",
    "        real_membership,\n",
    "        real_non_membership,\n",
    "        total_power_membership,\n",
    "        total_power_non_membership,\n",
    "    )\n",
    "\n",
    "\n",
    "# 感情を分類する関数\n",
    "def classify_sentiment(real_and_power):\n",
    "    (\n",
    "        real_membership,\n",
    "        real_non_membership,\n",
    "        total_power_membership,\n",
    "        total_power_non_membership,\n",
    "    ) = real_and_power\n",
    "\n",
    "    # 角周波数振幅を使用して分類\n",
    "    angular_frequency_membership = np.abs(real_membership)\n",
    "    angular_frequency_non_membership = np.abs(real_non_membership)\n",
    "\n",
    "    # パワー項の合計と角周波数振幅を基に感情を分類\n",
    "    if (angular_frequency_membership[1] > angular_frequency_non_membership[1]) or (\n",
    "        total_power_membership > total_power_non_membership\n",
    "    ):\n",
    "        return 1  # ポジティブ\n",
    "    else:\n",
    "        return 0  # ネガティブ\n",
    "\n",
    "\n",
    "# データフレームに品詞タグ付け、感情スコア、およびTIFSの結果を追加\n",
    "train_df[\"pos_tags\"] = train_df[\"text\"].apply(pos_tagging)\n",
    "train_df[\"sentiment_scores\"] = train_df[\"pos_tags\"].apply(sentiment_analysis)\n",
    "train_df[\"tifs\"] = train_df[\"sentiment_scores\"].apply(calculate_tifs)\n",
    "\n",
    "# FFTの結果をデータフレームに追加\n",
    "train_df[\"fft_results\"] = train_df[\"tifs\"].apply(apply_fft)\n",
    "\n",
    "# 実数部分とパワー項をデータフレームに追加\n",
    "train_df[\"real_and_power\"] = train_df[\"fft_results\"].apply(calculate_real_and_power)\n",
    "\n",
    "# 感情を分類\n",
    "train_df[\"predicted_label\"] = train_df[\"real_and_power\"].apply(classify_sentiment)\n",
    "\n",
    "# 精度とF1スコアを計算\n",
    "accuracy = accuracy_score(train_df[\"label\"], train_df[\"predicted_label\"])\n",
    "f1 = f1_score(train_df[\"label\"], train_df[\"predicted_label\"])\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
